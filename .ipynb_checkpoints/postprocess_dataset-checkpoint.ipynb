{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd492156-dc75-4f57-b634-2443340d7a52",
   "metadata": {},
   "source": [
    "## Create dataset for Margiotta et al., 2026\n",
    "#### CLM5.1 LAI PPE with independent PFT parameter perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cd2e6f2-502b-4c78-9cdd-45fbd02d10d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4113da42-620c-4bac-902a-79741b165812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/linnia/conda-envs/mlenv/lib/python3.10/site-packages/dask_jobqueue/pbs.py:82: FutureWarning: project has been renamed to account as this kwarg was used wit -A option. You are still using it (please also check config files). If you did not set account yet, project will be respected for now, but it will be removed in a future release. If you already set account, project is ignored and you can remove it.\n",
      "  warnings.warn(warn, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Setup your PBSCluster\n",
    "import dask\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "ncores=1\n",
    "nmem='10GB'\n",
    "cluster = PBSCluster(\n",
    "    cores=ncores, # The number of cores you want\n",
    "    memory=nmem, # Amount of memory\n",
    "    processes=1, # How many processes\n",
    "    queue='casper', # The type of queue to utilize (/glade/u/apps/dav/opt/usr/bin/execcasper)\n",
    "    local_directory='$TMPDIR', # Use your local directory\n",
    "    resource_spec='select=1:ncpus='+str(ncores)+':mem='+nmem, # Specify resources\n",
    "    project='P93300641', # Input your project ID here\n",
    "    walltime='02:00:00', # Amount of wall time\n",
    "    #interface='ib0', # Interface to use\n",
    ")\n",
    "\n",
    "# Scale up\n",
    "cluster.scale(5)\n",
    "\n",
    "# Setup your client\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcfdc7f-dd02-4eff-bde7-c0d7b2076eb5",
   "metadata": {},
   "source": [
    "#### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f1369e4-e279-45eb-b8f1-dc57a796ea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_files(exp,tape='h0',yy=()):\n",
    "\n",
    "    top='/glade/campaign/cgd/tss/projects/PPE/PPEn11_OAAT/'\n",
    "    d=top+exp+'/hist/'\n",
    "    if exp=='transient':\n",
    "        d='/glade/campaign/cgd/tss/projects/PPE/PPEn11_LHC/transient/hist/'\n",
    "    elif exp=='pftLAI':\n",
    "        d='/glade/work/linnia/CLM-PPE-LAI_tests/exp2_pftLAI/data/hist/'\n",
    "    elif exp=='SSP370':\n",
    "        d='/glade/campaign/cgd/tss/projects/PPE/PPEn11_LHC/SSP370/hist/'\n",
    "    elif exp=='wave1':\n",
    "        d='/glade/campaign/cgd/tss/projects/PPE/PPEn14_wave1/transient/hist/'\n",
    "    elif exp=='wave2':\n",
    "        d='/glade/campaign/cgd/tss/projects/PPE/PPEn14_wave2/transient/hist/'\n",
    "        \n",
    "    oaats=['CTL2010','C285','C867','AF1855','AF2095','NDEP']\n",
    "    key={oaat:'/glade/campaign/asp/djk2120/PPEn11/csvs/surv.csv' for oaat in oaats}\n",
    "    yys={oaat:(2005,2014) for oaat in oaats}\n",
    "    key['transient']='/glade/campaign/asp/djk2120/PPEn11/csvs/lhc220926.txt'\n",
    "    yys['transient']=(1850,2014)\n",
    "    #LRH: need to make path an input to function\n",
    "    key['EmBE']='/glade/work/linnia/CLM-PPE-LAI_tests/exp1_EmBE/psets_exp1_EmBE_230419.txt' \n",
    "    yys['EmBE']=(1850,2014)\n",
    "    key['pftLAI']='/glade/work/linnia/CLM-PPE-LAI_tests/exp2_pftLAI/pftLAI.txt'\n",
    "    yys['pftLAI']=(1850,2014)\n",
    "    key['SSP370']='/glade/campaign/asp/djk2120/PPEn11/csvs/lhc220926.txt'\n",
    "    yys['SSP370']=(2015,2099)\n",
    "    key['wave1']='/glade/work/linnia/CLM-PPE-LAI_tests/wave1/wave1.txt'\n",
    "    yys['wave1']=(1850,2014)\n",
    "    key['wave2']='/glade/work/linnia/CLM-PPE-LAI_tests/wave2/wave2.txt'\n",
    "    yys['wave2']=(1850,2014)\n",
    "    \n",
    "    df=pd.read_csv(key[exp])  \n",
    "    if not yy:\n",
    "        yr0,yr1=yys[exp]\n",
    "    else:\n",
    "        yr0,yr1=yy\n",
    "\n",
    "    if exp=='transient' or exp=='SSP370' or exp=='EmBE' or exp=='pftLAI' or exp=='wave1' or exp=='wave2': #LRH\n",
    "        keys = df.member.values\n",
    "        appends={}\n",
    "        params=[]\n",
    "        for p in df.keys():\n",
    "            if p!='member':\n",
    "                appends[p]=xr.DataArray(np.concatenate(([np.nan],df[p].values)),dims='ens')\n",
    "                params.append(p)\n",
    "        appends['params']=xr.DataArray(params,dims='param')\n",
    "        if exp=='transient' or exp=='SSP370': #LRH\n",
    "            keys=np.concatenate((['LHC0000'],keys))\n",
    "        if exp=='EmBE': #LRH\n",
    "            keys=np.concatenate((['exp1_EmBE0001'],keys))\n",
    "        if exp=='wave1':\n",
    "            keys=np.concatenate((['wave10001'],keys))\n",
    "        if exp=='wave2':\n",
    "            keys=np.concatenate((['wave20001'],keys))\n",
    "        appends['key']=xr.DataArray(keys,dims='ens')\n",
    "\n",
    "    else:\n",
    "        keys=df.key.values\n",
    "        appends={v:xr.DataArray(df[v].values,dims='ens') for v in ['key','param','minmax']}\n",
    "       \n",
    "    fs   = np.array(sorted(glob.glob(d+'*'+tape+'*')))\n",
    "    yrs  = np.array([int(f.split(tape)[1][1:5]) for f in fs])\n",
    "    \n",
    "    #bump back yr0, if needed\n",
    "    uyrs=np.unique(yrs)\n",
    "    yr0=uyrs[(uyrs/yr0)<=1][-1]\n",
    "\n",
    "    #find index to subset files\n",
    "    ix    = (yrs>=yr0)&(yrs<=yr1)\n",
    "    fs    = fs[ix] \n",
    "\n",
    "    #organize files to match sequence of keys\n",
    "    ny=len(np.unique(yrs[ix]))\n",
    "    \n",
    "    if exp=='transient' or exp=='SSP370':\n",
    "        fkeys=np.array([f.split(exp+'_')[1].split('.')[0] for f in fs])\n",
    "    elif exp=='EmBE' or exp=='wave1' or exp=='wave2':\n",
    "        fkeys=np.array([f.split('transient_')[1].split('.')[0] for f in fs]) #LRH\n",
    "    else:\n",
    "        fkeys=np.array([f.split(exp+'_')[1].split('.')[0] for f in fs]) #LRH\n",
    "\n",
    "    if ny==1:\n",
    "        files=[fs[fkeys==k][0] for k in keys]\n",
    "        dims  = 'ens'\n",
    "    else:\n",
    "        files=[list(fs[fkeys==k]) for k in keys]\n",
    "        dims  = ['ens','time']\n",
    "\n",
    "    #add landarea information\n",
    "    if exp=='transient' or 'SSP370' or 'EmBE' or 'pftLAI' or 'wave1' or 'wave2': #LRH\n",
    "        fla='landarea_transient.nc'\n",
    "    else:\n",
    "        fla='landarea_oaat.nc'\n",
    "    la=xr.open_dataset(fla)\n",
    "    appends['la']=la.landarea\n",
    "    if tape=='h1':\n",
    "        appends['lapft']=la.landarea_pft\n",
    "        \n",
    "    return files,appends,dims\n",
    "\n",
    "\n",
    "def get_ds(files,dims,dvs=[],appends={},singles=[]):\n",
    "    if dvs:\n",
    "        def preprocess(ds):\n",
    "            return ds[dvs]\n",
    "    else:\n",
    "        def preprocess(ds):\n",
    "            return ds\n",
    "\n",
    "    ds = xr.open_mfdataset(files,combine='nested',concat_dim=dims,\n",
    "                           parallel=True,\n",
    "                           preprocess=preprocess)\n",
    "    f=np.array(files).ravel()[0]\n",
    "    htape=f.split('clm2')[1][1:3]\n",
    "\n",
    "    #add extra variables\n",
    "    tmp = xr.open_dataset(f)\n",
    "    for v in tmp.data_vars:\n",
    "        if 'time' not in tmp[v].dims: \n",
    "            if v not in ds:\n",
    "                ds[v]=tmp[v]\n",
    "    \n",
    "    #fix up time dimension, swap pft\n",
    "    if (htape=='h0')|(htape=='h1'):\n",
    "        yr0=str(ds['time.year'][0].values)\n",
    "        nt=len(ds.time)\n",
    "        ds['time'] = xr.cftime_range(yr0,periods=nt,freq='MS',calendar='noleap') #fix time bug\n",
    "    if (htape=='h1'):\n",
    "        ds['pft']=ds['pfts1d_itype_veg']\n",
    "        \n",
    "    \n",
    "    for append in appends:\n",
    "        ds[append]=appends[append]\n",
    "        \n",
    "             \n",
    "    return ds\n",
    "\n",
    "def get_exp(exp,dvs=[],tape='h0',yy=(),defonly=False):\n",
    "    '''\n",
    "    exp: 'SSP370','transient','CTL2010','C285','C867','AF1855','2095','NDEP'\n",
    "    dvs:  e.g. ['TLAI']    or [] returns all available variables\n",
    "    tape: 'h0','h1',etc.\n",
    "    yy:   e.g. (2005,2014) or () returns all available years\n",
    "    '''\n",
    "    files,appends,dims=get_files(exp,tape=tape,yy=yy)\n",
    "    \n",
    "    if defonly:\n",
    "        files=files[0]\n",
    "        dims='time'\n",
    "\n",
    "    ds=get_ds(files,dims,dvs=dvs,appends=appends)\n",
    "    \n",
    "    f,a,d=get_files(exp,tape='h0',yy=yy)\n",
    "    singles=['RAIN','SNOW','TSA','RH2M','FSDS','WIND','TBOT','QBOT','FLDS']\n",
    "    tmp=get_ds(f[0],'time',dvs=singles)\n",
    "    for s in singles:\n",
    "        ds[s]=tmp[s]\n",
    "    \n",
    "    if len(yy)>0:  \n",
    "        ds=ds.sel(time=slice(str(yy[0]),str(yy[1])))\n",
    "    \n",
    "    ds['PREC']=ds.RAIN+ds.SNOW\n",
    "    \n",
    "    t=ds.TSA-273.15\n",
    "    rh=ds.RH2M/100\n",
    "    es=0.61094*np.exp(17.625*t/(t+234.04))\n",
    "    ds['VPD']=((1-rh)*es).compute()\n",
    "    ds['VPD'].attrs={'long_name':'vapor pressure deficit','units':'kPa'}\n",
    "    ds['VP']=(rh*es).compute()\n",
    "    ds['VP'].attrs={'long_name':'vapor pressure','units':'kPa'}\n",
    "    \n",
    "    whit = xr.open_dataset('./whit/whitkey.nc')\n",
    "    ds['biome']=whit.biome\n",
    "    ds['biome_name']=whit.biome_name\n",
    "                \n",
    "    #get the pft names\n",
    "    pfts=xr.open_dataset('/glade/campaign/asp/djk2120/PPEn11/paramfiles/OAAT0000.nc').pftname\n",
    "    pfts=[str(p)[2:-1].strip() for p in pfts.values][:17]\n",
    "    ds['pft_name']=xr.DataArray(pfts,dims='pft_id')\n",
    "    \n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4d72e4-cbad-4dcd-9446-c5658d1daaf6",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c75366ea-e3af-45cd-a21e-336e0980fad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/linnia/tmp/ipykernel_65758/2809491194.py:118: FutureWarning: In a future version of xarray decode_timedelta will default to False rather than None. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  tmp = xr.open_dataset(f)\n",
      "/glade/derecho/scratch/linnia/tmp/ipykernel_65758/2809491194.py:178: FutureWarning: In a future version of xarray decode_timedelta will default to False rather than None. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  pfts=xr.open_dataset('/glade/campaign/asp/djk2120/PPEn11/paramfiles/OAAT0000.nc').pftname\n"
     ]
    }
   ],
   "source": [
    "ds_wave1=get_exp('wave1',dvs=['GPP','BTRANMN','FCTR','TOTVEGC'],tape='h1',yy=(1985,2014))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "89aeee20-4c6e-49e2-976f-6eb7c3310022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### add parameter setting info \n",
    "key = '/glade/u/home/linnia/clm5ppe/pyth/wave1/wave1_params.nc'\n",
    "params = xr.open_dataset(key)\n",
    "params = params.rename({'pft':'pft_name','wave1_params':'parameter_values_normalized'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f69ebf8d-8e43-4bf5-8ccd-b07e046bbe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data = ds_wave1.isel(ens=slice(1,501))\n",
    "out_data = out_data.assign_coords(pft_name=params['pft_name'])\n",
    "out_data['parameter_values_normalized'] = params['parameter_values_normalized']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef65dd32-58d8-4001-82d4-14137a772b3a",
   "metadata": {},
   "source": [
    "#### Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d94e758-d49b-490e-9ac4-c2ece039b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add metadata\n",
    "ds.attrs= {'created_by': 'linnia lh3194@columbia.edu', \n",
    "           'created_on': '07/18/2025',\n",
    "           'created_with': '/glade/u/home/linnia/clm5-pft-ppe/postprocess_data.ipynb',\n",
    "           'github_repo': 'https://github.com/linniahawkins/clm5-pft-ppe'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc1899-0cec-40f1-b971-95b169a6041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data.to_netcdf(\"/glade/derecho/scratch/linnia/for_Evan/clm5-pft-ppe_dataset_test.nc\", format='NETCDF4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlenv]",
   "language": "python",
   "name": "conda-env-mlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
